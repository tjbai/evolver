{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sanity check overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from de import DependencyEvolver\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = DependencyEvolver(\n",
    "    d_model=16,\n",
    "    dim_feedforward=8,\n",
    "    nhead=1,\n",
    "    dropout=0,\n",
    "    N=5,\n",
    "    encoder_layers=1,\n",
    "    decoder_layers=1,\n",
    "    tok_v=tokenizer.vocab_size,\n",
    "    rel_v=2,\n",
    "    pos_v=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "batch = (\n",
    "    (8823, 2),\n",
    "    [\n",
    "        torch.tensor([[[1, 0, 2, 0, 3]]]),\n",
    "        torch.tensor([[[-1, -1, 1, -1, -1]]]),\n",
    "        torch.tensor([[[-1, 2, -1, 2, -1]]]),\n",
    "        torch.tensor([[[-1, 0, -1, 1, -1]]]),\n",
    "        torch.tensor([[[-1, 0, -1, 1, -1]]]),\n",
    "        torch.tensor([[[-1, 2016, -1, 2833, -1]]])\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_loader = [batch for _ in range(1000)]\n",
    "eval_loader = [batch for _ in range(20)]\n",
    "optim = AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._train(optim, train_loader, eval_loader, len(train_loader), 20, 1, 1e9, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## full data creation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "\n",
    "with open('./data/ud/en_gum-ud-train.conllu', 'r') as f:\n",
    "    train_sentences = conllu.parse(f.read())\n",
    "    \n",
    "with open('./data/ud/en_gum-ud-dev.conllu', 'r') as f:\n",
    "    dev_sentences = conllu.parse(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set()\n",
    "for sentence in train_sentences + dev_sentences:\n",
    "    for tok in sentence:\n",
    "        seen.add(tok['form'])\n",
    "\n",
    "print(len(seen))\n",
    "\n",
    "with open('vocab/gum_tok.vocab', 'w') as f:\n",
    "    for tok in seen:\n",
    "        f.write(tok)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct adjacent sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def family_tree(parsed):\n",
    "    children = {tok['id']: [] for tok in parsed}\n",
    "    for tok in parsed:\n",
    "        if tok['head'] is None: return None\n",
    "        if tok['head'] != 0: children[tok['head']].append(tok['id'])\n",
    "        \n",
    "    i, root = next((i, tok) for (i, tok) in enumerate(parsed) if tok['head'] == 0)\n",
    "\n",
    "    seqs = [[(root['form'], root['upos'], root['deprel'], i, True, -1)]]\n",
    "    cur_leaves = [root['id']]\n",
    "    all_leaves = [root['id']]\n",
    "\n",
    "    while cur_leaves:\n",
    "        seq = []\n",
    "        next_leaves = []\n",
    "        \n",
    "        for i, tok in enumerate(parsed):\n",
    "            if tok['id'] in all_leaves or tok['head'] in all_leaves:\n",
    "                seq.append((\n",
    "                    tok['form'], tok['upos'], tok['deprel'],\n",
    "                    i, tok['head'] in cur_leaves, None\n",
    "                ))\n",
    "                if tok['head'] in cur_leaves: next_leaves.extend(children[tok['id']])\n",
    "                \n",
    "        for i, (form, upos, deprel, j, is_leaf, _) in enumerate(seq):\n",
    "            tok = next(t for t in parsed if t['form'] == form and t['upos'] == upos and t['deprel'] == deprel)\n",
    "            if tok['head'] == 0:\n",
    "                par = -1\n",
    "            else:\n",
    "                par = next((j for j, (p_form, p_upos, p_deprel, _, _, _) in enumerate(seq)\n",
    "                            if p_form == parsed[tok['head']-1]['form']\n",
    "                            and p_upos == parsed[tok['head']-1]['upos']\n",
    "                            and p_deprel == parsed[tok['head']-1]['deprel']),\n",
    "                            None)\n",
    "            \n",
    "            seq[i] = (form, upos, deprel, j, is_leaf, par)\n",
    "        \n",
    "        seqs.append(seq)\n",
    "        cur_leaves = next_leaves\n",
    "        all_leaves.extend(next_leaves)\n",
    "        \n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_map = {}\n",
    "rel_map = {}\n",
    "pos_map = {}\n",
    "\n",
    "with open('vocab/gum_tok.vocab', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        tok_map[line.strip()] = i\n",
    "\n",
    "with open('vocab/rel.vocab', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        rel_map[line.strip()] = len(tok_map) + i\n",
    "        \n",
    "with open('vocab/pos.vocab', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        pos_map[line.strip()] = len(tok_map) + len(rel_map) + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tok_map), len(rel_map), len(pos_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode = {v: k for k, v in (list(tok_map.items()) + list(rel_map.items()) + list(pos_map.items()))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from de import INS_ID, CPY_ID, PRO_ID, EOS_ID\n",
    "\n",
    "def label(seqs, N):\n",
    "    traj = [[] for _ in range(6)]\n",
    "\n",
    "    for i in range(1, len(seqs)-1):\n",
    "        a, b = seqs[i], seqs[i+1]\n",
    "    \n",
    "        op_list = [CPY_ID]\n",
    "        cpy_list = [-1]\n",
    "        par_list = [-1]\n",
    "        tok_list = [-1]\n",
    "        pos_list = [-1]\n",
    "        rel_list = [-1]\n",
    "        \n",
    "        prev = {i: is_leaf for (_, _, _, i, is_leaf, _) in a}\n",
    "        for i, (tok, pos, rel, j, _, par) in enumerate(b[:N-2]):\n",
    "            if j in prev:\n",
    "                op_list.append(PRO_ID if prev[j] else CPY_ID)\n",
    "                cpy_list.append(next(i for i, t in enumerate(a) if t[3] == j) + 1)\n",
    "                par_list.append(-1)\n",
    "                tok_list.append(-1)\n",
    "                rel_list.append(-1)\n",
    "                pos_list.append(-1)\n",
    "            else:\n",
    "                op_list.append(INS_ID)\n",
    "                cpy_list.append(-1)\n",
    "                par_list.append(par + 1)\n",
    "                tok_list.append(tok_map.get(tok, len(tok_map) + len(rel_map) + len(pos_map)))\n",
    "                rel_list.append(rel_map[rel])\n",
    "                pos_list.append(pos_map[pos])\n",
    "                \n",
    "        op_list.append(EOS_ID)\n",
    "        for i, list in enumerate([op_list, cpy_list, par_list, rel_list, pos_list, tok_list]):\n",
    "            list.extend([-1 for _ in range(N-len(list))])\n",
    "            traj[i].append(list)\n",
    "            \n",
    "    root = (tok_map[seqs[0][0][0]], rel_map[seqs[0][0][2]], pos_map[seqs[0][0][1]])\n",
    "            \n",
    "    return root, traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "traj_list = []\n",
    "t1 = t2 = t3 = 0\n",
    "\n",
    "for parsed in tqdm(train_sentences):\n",
    "    seqs = family_tree(parsed)\n",
    "    \n",
    "    # missing head\n",
    "    if seqs is None:\n",
    "        t1 += 1\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        root, labels = label(seqs, 64)\n",
    "        \n",
    "        # tree depth is 1\n",
    "        if len(labels[0]) == 0:\n",
    "            t3 += 1\n",
    "            continue\n",
    "        \n",
    "        traj_list.append((root, labels))\n",
    "       \n",
    "    # missing parent\n",
    "    except TypeError as e:\n",
    "        t2 += 1\n",
    "        continue\n",
    "    \n",
    "print(len(traj_list))\n",
    "    \n",
    "import pickle\n",
    "with open('data/gum/train_1.pkl', 'wb') as f:\n",
    "    pickle.dump(traj_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = single_loader('data/gum/train_1.pkl')\n",
    "root, tgts = next(iter(loader))\n",
    "\n",
    "print(decode[root[0]], decode[root[1]], decode[root[2]])\n",
    "print(tgts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root, tgts = next(iter(loader))\n",
    "\n",
    "model.traj_loss(root, tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from de import *\n",
    "from torch.optim import AdamW\n",
    "\n",
    "model = DependencyEvolver(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    encoder_layers=6,\n",
    "    decoder_layers=6,\n",
    "    N=64 \n",
    ")\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=1e-3)\n",
    "loader = single_loader('data/gum/train_1.pkl', shuffle=False)\n",
    "\n",
    "model._train(\n",
    "    optim, loader, loader,\n",
    "    1000, 20, 4,\n",
    "    1001, 100\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
