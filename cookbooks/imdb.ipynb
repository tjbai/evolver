{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def noise(seq):\n",
    "    traj = [] \n",
    "    toks = tokenizer(seq, max_length=512, truncation=True)['input_ids'][1:] # remove CLS token\n",
    "    N = len(toks)\n",
    "    \n",
    "    while toks:\n",
    "        toks = toks[:len(toks)//2]\n",
    "        traj.append(tokenizer.decode(toks))\n",
    "        \n",
    "    return traj[::-1], 0, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "lengths = []\n",
    "\n",
    "with open('../data/imdb/imdb_train_1.jsonl', 'w') as train_f, open('../data/imdb/imdb_dev_1.jsonl', 'w') as dev_f:\n",
    "    for i, file in tqdm(enumerate(os.listdir('../data/imdb/raw/train/neg'))):\n",
    "        df = train_f if i < 10000 else dev_f\n",
    "        with open(f'../data/imdb/raw/train/neg/{file}', 'r') as f:\n",
    "            seq = f.read()\n",
    "            *traj, N = noise(seq)\n",
    "            json.dump(traj, df)\n",
    "            df.write('\\n')\n",
    "            lengths.append(N)\n",
    "            \n",
    "    for i, file in tqdm(enumerate(os.listdir('../data/imdb/raw/train/pos'))):\n",
    "        df = train_f if i < 10000 else dev_f\n",
    "        with open(f'../data/imdb/raw/train/pos/{file}', 'r') as f:\n",
    "            seq = f.read()\n",
    "            *traj, N = noise(seq)\n",
    "            json.dump(traj, df)\n",
    "            df.write('\\n')\n",
    "            lengths.append(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertTokenizer\n",
    "from constants import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "with open('../data/imdb/imdb_train_1.jsonl', 'r') as f:\n",
    "    for i, line in tqdm(enumerate(f.readlines())):\n",
    "        if not line: continue\n",
    "        traj, _ = json.loads(line)\n",
    "        ids = tokenizer(traj)['input_ids']\n",
    "        \n",
    "        traj_op_tgts = []\n",
    "        traj_tok_tgts = []\n",
    "        traj_idx_tgts = []\n",
    "        \n",
    "        for j, seq in enumerate(ids[1:], start=1):\n",
    "            N = len(ids[j-1]) - 2\n",
    "            M = len(seq) - 2 - N\n",
    "            \n",
    "            op_tgts = [INS_ID] + [CPY_ID for _ in range(N)] + [INS_ID for _ in range(M)] + [EOS_ID]\n",
    "            tok_tgts = [BOS_TOKEN_ID] + [PAD_TOKEN_ID for _ in range(N)] + [tok for tok in seq[1+N:1+N+M]] + [PAD_TOKEN_ID]\n",
    "            idx_tgts = [0]  + [i for i in range(1, N+1)] + [0 for _ in range(M)] + [0]\n",
    "           \n",
    "            # pad out to 512 \n",
    "            op_tgts += [PAD_ID for _ in range(512-len(seq))]\n",
    "            tok_tgts += [PAD_TOKEN_ID for _ in range(512-len(seq))]\n",
    "            idx_tgts += [0 for _ in range(512-len(seq))]\n",
    "            \n",
    "            traj_op_tgts.append(op_tgts)\n",
    "            traj_tok_tgts.append(tok_tgts)\n",
    "            traj_idx_tgts.append(idx_tgts)\n",
    "            \n",
    "        traj_op_tgts = torch.tensor(traj_op_tgts)\n",
    "        traj_tok_tgts = torch.tensor(traj_tok_tgts)\n",
    "        traj_idx_tgts = torch.tensor(traj_idx_tgts)\n",
    "        \n",
    "        with open(f'../data/imdb/ezpz/sup-imdb-1_{i}.zst', 'wb') as f:\n",
    "            pickle.dump((traj_op_tgts, traj_tok_tgts, traj_idx_tgts), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "traj_op_tgts = F.one_hot(traj_op_tgts, 5)\n",
    "traj_tok_tgts = F.one_hot(traj_tok_tgts, VOCAB_SIZE)\n",
    "traj_idx_tgts = F.one_hot(traj_idx_tgts, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import elaborate\n",
    "\n",
    "elaborate((traj_op_tgts, traj_tok_tgts, traj_idx_tgts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
