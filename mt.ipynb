{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import spacy\n",
    "\n",
    "# dataset = datasets.load_dataset(\"wmt14\", \"cs-en\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'The world needs the World Bank a lot more than it needs another condominium.'\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "parse = nlp(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {s: i for i, s in enumerate(['BOS', 'EOS'] + list(nlp.vocab.strings) + list(spacy.glossary.GLOSSARY))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth(doc):\n",
    "    root = [tok for tok in doc if tok.head == tok] [0]\n",
    "    def dfs(node):\n",
    "        r = 1\n",
    "        for child in node.children: r = max(r, 1 + dfs(child))\n",
    "        return r\n",
    "    return dfs(root)\n",
    "\n",
    "depth(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(doc):\n",
    "    INS, CPY, SUB = 0, 1, 2 \n",
    "    \n",
    "    traj = [['_' for _ in range(len(doc))] for _ in range(2*depth(doc))]\n",
    "    \n",
    "    def traverse(token, depth):\n",
    "        for i in range(depth, len(traj)):\n",
    "            traj[i][token.i] = (token.text if (i > depth+1) else token.pos_, token.i, token.head.i)\n",
    "        \n",
    "        traj[depth+1][token.i] = (token.text, token.i, token.head.i)\n",
    "        \n",
    "        for child in token.children:\n",
    "            traverse(child, depth+2)\n",
    "    \n",
    "    root = next(token for token in doc if token.head == token)\n",
    "    traverse(root, 0)\n",
    "   \n",
    "    res = [[root.pos_]]\n",
    "    edit_traj = [[(INS, 'BOS', -1), (INS, root.pos_, -1), (INS, 'EOS', -1)]]\n",
    "    m = {}\n",
    "    \n",
    "    for i, seq in enumerate(traj[1:]):\n",
    "        cur_edits = [(CPY, -1, 0)]\n",
    "           \n",
    "        if i % 2 == 0:\n",
    "            k = 1\n",
    "            for t in seq:\n",
    "                if t == '_': continue\n",
    "                if t[1] in m:\n",
    "                    cur_edits.append((CPY, -1, k))\n",
    "                else:\n",
    "                    cur_edits.append((SUB, t[0], k))\n",
    "                m[t[1]] = k\n",
    "                k += 1\n",
    "                \n",
    "        else:\n",
    "            k = 1\n",
    "            for t in seq:\n",
    "                if t == '_': continue\n",
    "                if t[1] in m:\n",
    "                    cur_edits.append((CPY, -1, m[t[1]]))\n",
    "                else:\n",
    "                    cur_edits.append((SUB, t[0], m[t[2]]))\n",
    "           \n",
    "        res.append([t[0] for t in seq if t != '_'])         \n",
    "        cur_edits.append((CPY, -1, len(edit_traj[-1])+1))\n",
    "        edit_traj.append(cur_edits)\n",
    "    \n",
    "    return res, edit_traj\n",
    "\n",
    "traj, edit_traj = gen(parse)\n",
    "\n",
    "for t in traj:\n",
    "    print(' '.join(t))\n",
    "\n",
    "print()    \n",
    "for e in edit_traj:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTDataset, MTEditDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = MTEditDataset(split='train', max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ids, res, edit_traj = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import SpacyTokenizer\n",
    "tokenizer = SpacyTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import SpacyTokenizer\n",
    "\n",
    "tokenizer = SpacyTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTTransformer\n",
    "\n",
    "model = MTTransformer(\n",
    "    d_model=512,\n",
    "    dim_feedforward=2048,\n",
    "    nhead=8,\n",
    "    dropout=0.1,\n",
    "    layer_norm_eps=1e-5,\n",
    "    encoder_layers=4,\n",
    "    decoder_layers=4,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_len=128,\n",
    "    pad_token_id=tokenizer.vocab['PAD'],\n",
    "    bos_token_id=tokenizer.vocab['BOS'],\n",
    "    eos_token_id=tokenizer.vocab['EOS'],\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model.token_embedding.forward(torch.tensor([-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab/wmt14_de_en.vocab', 'r') as f:\n",
    "    print(len(f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = max(v for _, v in tokenizer.vocab.items())\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = load_dataset('wmt14', 'de-en')\n",
    "\n",
    "de_len = []\n",
    "en_len = []\n",
    "\n",
    "for pair in tqdm(dataset['train']['translation'][:5000]):\n",
    "    de_len.append(len(tokenizer.encode(pair['de'], lang='de')))\n",
    "    en_len.append(len(tokenizer.encode(pair['en'], lang='en')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(de_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(en_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mt.py --config=./configs/wmt/ar-toy.json --local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mt.py --config=./configs/wmt/evolver-toy.json --local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTTransformer, BertTokenizer, MTDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer()\n",
    "\n",
    "model = MTTransformer(\n",
    "    d_model=512, dim_feedforward=2048, nhead=8, dropout=0, layer_norm_eps=1e-5, encoder_layers=2, decoder_layers=2,\n",
    "    vocab_size=tokenizer.vocab_size, max_len=256, pad_token_id=tokenizer.pad_token_id, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "dataset = MTDataset(split='test', tokenizer=tokenizer)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTTransformer, BertTokenizer, MTDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer()\n",
    "\n",
    "model = MTTransformer(\n",
    "    d_model=512, dim_feedforward=2048, nhead=8, dropout=0, layer_norm_eps=1e-5, encoder_layers=2, decoder_layers=2,\n",
    "    vocab_size=tokenizer.vocab_size, max_len=256, pad_token_id=tokenizer.pad_token_id, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "dataset = MTDataset(split='test', tokenizer=tokenizer)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))\n",
    "\n",
    "batch['src_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# output = model.step(batch)\n",
    "\n",
    "output, cache = model(batch)\n",
    "\n",
    "print(output.shape)\n",
    "\n",
    "model.decoder.set_causal()\n",
    "\n",
    "output, cache = model(batch)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build static vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "de_nlp = spacy.load('de_core_news_sm')\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "en_toks = set()\n",
    "en_pos = set()\n",
    "de_toks = set()\n",
    "de_pos = set()\n",
    "\n",
    "dataset = load_dataset('wmt14', 'de-en')\n",
    "for split in ['train', 'test', 'validation']:\n",
    "    for pair in tqdm(dataset[split]['translation'], desc=f'crawling {split}'):\n",
    "        for de in de_nlp(pair['de']):\n",
    "            de_toks.add(de.text)\n",
    "            de_pos.add(de.pos_)\n",
    "        for en in en_nlp(pair['de']):\n",
    "            en_toks.add(en.text) \n",
    "            en_pos.add(en.pos_) \n",
    "    \n",
    "    print(f'crawled {len(en_toks)} en_toks, {len(en_pos)} en_pos, {len(de_toks)} de_toks, {len(de_pos)} de_pos')\n",
    "    vocab = en_toks.union(en_pos, de_toks, de_pos)\n",
    "    with open('vocab/wmt14_de_en.vocab', 'w') as f:\n",
    "        for v in tqdm(vocab, desc=f'dumping {split}'):\n",
    "            f.write(v)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq edit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTEditDataset\n",
    "\n",
    "dataset = MTEditDataset(split='test', max_len=128, buffer_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ids, input_ids, edit_ids = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mt import SpacyTokenizer\n",
    "\n",
    "tok = SpacyTokenizer()\n",
    "\n",
    "print(input_ids)\n",
    "print(edit_ids)\n",
    "\n",
    "print(tok.decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tok = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTDataset, BertTokenizer\n",
    "\n",
    "dataset = MTDataset(split='test', tokenizer=BertTokenizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTTransformer, SpacyTokenizer, MTDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = SpacyTokenizer()\n",
    "\n",
    "model = MTTransformer(\n",
    "    d_model=512, dim_feedforward=2048, nhead=8, dropout=0.1, layer_norm_eps=1e-5, encoder_layers=6, decoder_layers=6,\n",
    "    vocab_size=tokenizer.vocab_size, max_len=128, pad_token_id=tokenizer.pad_token_id, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id, name=None\n",
    ")\n",
    "\n",
    "dataset = MTDataset(split='test', tokenizer=tokenizer)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTEvolver, MTEditDataset, SpacyTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = SpacyTokenizer()\n",
    "\n",
    "model = MTEvolver(\n",
    "    d_model=512, dim_feedforward=2048, nhead=8, dropout=0.1, layer_norm_eps=1e-5, encoder_layers=6, decoder_layers=6,\n",
    "    vocab_size=tokenizer.vocab_size, max_len=128, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id, name=None\n",
    ")\n",
    "\n",
    "dataset = MTEditDataset(split='test', tokenizer=tokenizer)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "(op_probs, tok_probs, idx_probs), _ = model.forward(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(op_probs.shape)\n",
    "print(tok_probs.shape)\n",
    "print(idx_probs.shape)\n",
    "print(batch['edit_ids'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['src_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder.parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import evaluate\n",
    "\n",
    "evaluate(model, loader, 'cpu', 1, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
