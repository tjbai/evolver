{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "parse = nlp('Reform number one, of course, is to ensure that the next World Bank President is not an American.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {s: i for i, s in enumerate(['BOS', 'EOS'] + list(nlp.vocab.strings) + list(spacy.glossary.GLOSSARY))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth(doc):\n",
    "    root = [tok for tok in doc if tok.head == tok] [0]\n",
    "    def dfs(node):\n",
    "        r = 1\n",
    "        for child in node.children: r = max(r, 1 + dfs(child))\n",
    "        return r\n",
    "    return dfs(root)\n",
    "\n",
    "depth(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(doc):\n",
    "    INS, CPY, SUB = 0, 1, 2 \n",
    "    \n",
    "    traj = [['_' for _ in range(len(doc))] for _ in range(2*depth(doc))]\n",
    "    \n",
    "    def traverse(token, depth):\n",
    "        for i in range(depth, len(traj)):\n",
    "            traj[i][token.i] = (token.text if (i > depth+1) else token.pos_, token.i, token.head.i)\n",
    "        \n",
    "        traj[depth+1][token.i] = (token.text, token.i, token.head.i)\n",
    "        \n",
    "        for child in token.children:\n",
    "            traverse(child, depth+2)\n",
    "    \n",
    "    root = next(token for token in doc if token.head == token)\n",
    "    traverse(root, 0)\n",
    "   \n",
    "    res = [[root.pos_]]\n",
    "    edit_traj = [[(INS, 'BOS', -1), (INS, root.pos_, -1), (INS, 'EOS', -1)]]\n",
    "    m = {}\n",
    "    \n",
    "    for i, seq in enumerate(traj[1:]):\n",
    "        cur_edits = [(CPY, -1, 0)]\n",
    "           \n",
    "        if i % 2 == 0:\n",
    "            k = 1\n",
    "            for t in seq:\n",
    "                if t == '_': continue\n",
    "                if t[1] in m:\n",
    "                    cur_edits.append((CPY, -1, k))\n",
    "                else:\n",
    "                    cur_edits.append((SUB, t[0], k))\n",
    "                m[t[1]] = k\n",
    "                k += 1\n",
    "                \n",
    "        else:\n",
    "            k = 1\n",
    "            for t in seq:\n",
    "                if t == '_': continue\n",
    "                if t[1] in m:\n",
    "                    cur_edits.append((CPY, -1, m[t[1]]))\n",
    "                else:\n",
    "                    cur_edits.append((SUB, t[0], m[t[2]]))\n",
    "           \n",
    "        res.append([t[0] for t in seq if t != '_'])         \n",
    "        cur_edits.append((CPY, -1, len(edit_traj[-1])+1))\n",
    "        edit_traj.append(cur_edits)\n",
    "    \n",
    "    return res, edit_traj\n",
    "\n",
    "traj, edit_traj = gen(parse)\n",
    "\n",
    "for t in traj:\n",
    "    print(' '.join(t))\n",
    "\n",
    "print()    \n",
    "for e in edit_traj:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## figure out alignment!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
    "ids = tokenizer.encode(str(parse))\n",
    "tokenizer.decode(ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens = [str(t).lower() for t in parse]\n",
    "\n",
    "marian_tokens = [\n",
    "    t[1:].lower() if ord(t[0]) == 9601 else t.lower()\n",
    "    for t in tokenizer.tokenize(str(parse))\n",
    "]\n",
    "\n",
    "# we can check and remove if the first character is ord = 9601\n",
    "\n",
    "print(spacy_tokens)\n",
    "print(marian_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = load_dataset('wmt14', 'de-en', split='test')\n",
    "\n",
    "with open('tmp', 'w') as f:\n",
    "    for pair in tqdm(dataset['translation']):\n",
    "        en = pair['en']\n",
    "        parse = nlp(en)\n",
    "        \n",
    "        spacy_tokens = [str(t).lower() for t in parse]\n",
    "        \n",
    "        marian_tokens = [\n",
    "            t[1:].lower() if ord(t[0]) == 9601 else t.lower()\n",
    "            for t in tokenizer.tokenize(str(parse))]\n",
    "       \n",
    "        f.write(json.dumps(spacy_tokens))\n",
    "        f.write('\\n')\n",
    "        f.write(json.dumps(marian_tokens))\n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mt.py --config=./configs/wmt/ar-toy.json --local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mt.py --config=./configs/wmt/evolver-toy.json --local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mt.py --config=./configs/wmt/teacher-toy.json --local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTTransformer, BertTokenizer, MTEditDataset, SpacyTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = SpacyTokenizer()\n",
    "\n",
    "model = MTTransformer(\n",
    "    d_model=512, dim_feedforward=2048, nhead=8, dropout=0, layer_norm_eps=1e-5, encoder_layers=2, decoder_layers=2,\n",
    "    vocab_size=tokenizer.vocab_size, max_len=256, pad_token_id=tokenizer.pad_token_id, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "dataset = MTEditDataset(split='test', tokenizer=tokenizer)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for _ in tqdm(loader):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTTransformer, BertTokenizer, MTDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer()\n",
    "\n",
    "model = MTTransformer(\n",
    "    d_model=256, dim_feedforward=1024, nhead=8, dropout=0, layer_norm_eps=1e-5, encoder_layers=2, decoder_layers=2,\n",
    "    vocab_size=tokenizer.vocab_size, max_len=256, pad_token_id=tokenizer.pad_token_id, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "dataset = MTDataset(split='test', tokenizer=tokenizer)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build static vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "de_nlp = spacy.load('de_core_news_sm')\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "en_toks = set()\n",
    "en_pos = set()\n",
    "de_toks = set()\n",
    "de_pos = set()\n",
    "\n",
    "dataset = load_dataset('wmt14', 'de-en')\n",
    "for split in ['train', 'test', 'validation']:\n",
    "    for pair in tqdm(dataset[split]['translation'], desc=f'crawling {split}'):\n",
    "        for de in de_nlp(pair['de']):\n",
    "            de_toks.add(de.text)\n",
    "            de_pos.add(de.pos_)\n",
    "        for en in en_nlp(pair['de']):\n",
    "            en_toks.add(en.text) \n",
    "            en_pos.add(en.pos_) \n",
    "    \n",
    "    print(f'crawled {len(en_toks)} en_toks, {len(en_pos)} en_pos, {len(de_toks)} de_toks, {len(de_pos)} de_pos')\n",
    "    vocab = en_toks.union(en_pos, de_toks, de_pos)\n",
    "    with open('vocab/wmt14_de_en.vocab', 'w') as f:\n",
    "        for v in tqdm(vocab, desc=f'dumping {split}'):\n",
    "            f.write(v)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq edit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTEditDataset\n",
    "\n",
    "dataset = MTEditDataset(split='test', max_len=128, buffer_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ids, input_ids, edit_ids = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mt import SpacyTokenizer\n",
    "\n",
    "tok = SpacyTokenizer()\n",
    "\n",
    "print(input_ids)\n",
    "print(edit_ids)\n",
    "\n",
    "print(tok.decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tok = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTDataset, BertTokenizer\n",
    "\n",
    "dataset = MTDataset(split='test', tokenizer=BertTokenizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debug transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MarianTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = MarianTokenizer()\n",
    "\n",
    "dataset = load_dataset('wmt14', 'de-en', split='test')\n",
    "\n",
    "len_de = []\n",
    "len_en = []\n",
    "\n",
    "for thing in tqdm(dataset['translation']):\n",
    "    de = thing['de']\n",
    "    en = thing['en']\n",
    "    len_de.append(len(tokenizer.encode(de)))\n",
    "    len_en.append(len(tokenizer.encode(en)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(x):\n",
    "    x['translation']  = x['translation'][:4]\n",
    "\n",
    "dataset.map(truncate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = MTDataset(split='test', truncate=4)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=4, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(len_de)\n",
    "plt.hist(len_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
