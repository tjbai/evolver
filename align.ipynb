{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from transformers import MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to make sure the character-based span matching will _actually_ match correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spans(text, tokens):\n",
    "    spans = []\n",
    "    i = 0\n",
    "    for token in tokens:\n",
    "        while i < len(text) and text[i].isspace(): i += 1\n",
    "        if i < len(text):\n",
    "            s = i\n",
    "            i = text.find(token, i) + len(token)\n",
    "            spans.append((s, i))\n",
    "    \n",
    "    assert len(spans) == len(tokens), f'number of spans does not match number of tokens for: {text}'\n",
    "    return spans\n",
    "\n",
    "def get_alignments(text, spacy_tokens, marian_tokens):\n",
    "    spacy_spans = get_spans(text, spacy_tokens)\n",
    "    marian_spans = get_spans(text, marian_tokens)\n",
    "    alignment = {} # map marian_tokens[i] to spacy_tokens[j]\n",
    "    best_overlap = defaultdict(int) # track max overlap\n",
    "    \n",
    "    # just bruteforce check (who needs DP?)\n",
    "    for i, marian_span in enumerate(marian_spans):\n",
    "        for j, spacy_span in enumerate(spacy_spans):\n",
    "            overlap = max(0, min(spacy_span[1], marian_span[1]) - max(spacy_span[0], marian_span[0]))\n",
    "            if overlap > 0 and overlap > best_overlap[i]:\n",
    "                alignment[i] = j\n",
    "                best_overlap[i] = overlap\n",
    "                \n",
    "    for i, tok in enumerate(marian_tokens):\n",
    "        if tok == '': alignment[i] = alignment[i+1] # word break\n",
    "        if tok == '<unk>': alignment[i] = 1 + alignment[i-1] # unk\n",
    "    \n",
    "    assert len(alignment) == len(marian_tokens), f'did not find a complete alignment for: {text}'\n",
    "    return alignment\n",
    "\n",
    "marian = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "wmt = load_dataset('wmt14', 'de-en', split='validation')\n",
    "\n",
    "def get_spacy_tokens(text):\n",
    "    return [token.text for token in nlp(text)]\n",
    "\n",
    "def get_marian_tokens(text):\n",
    "    return [marian.decode(id) for id in marian(text)['input_ids'][:-1]]\n",
    "\n",
    "for i, example in enumerate(tqdm(wmt['translation'])):\n",
    "    # text = example['en']\n",
    "    text = example['de']\n",
    "    spacy_tokens = get_spacy_tokens(text)\n",
    "    marian_tokens = get_marian_tokens(text)\n",
    "    alignments = get_alignments(text, spacy_tokens, marian_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.en_nlp = spacy.load('en_core_web_sm')\n",
    "        self.de_nlp = spacy.load('de_core_news_sm')\n",
    "        self.marian = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
    "    \n",
    "    def get_spans(self, text, tokens):\n",
    "        spans = []\n",
    "        i = 0\n",
    "        for token in tokens:\n",
    "            while i < len(text) and text[i].isspace(): i += 1\n",
    "            if i < len(text):\n",
    "                s = i\n",
    "                i = text.find(token, i) + len(token)\n",
    "                spans.append((s, i))\n",
    "        \n",
    "        assert len(spans) == len(tokens), f'number of spans does not match number of tokens for: {text}'\n",
    "        return spans\n",
    "\n",
    "    def get_alignment(self, text, spacy_tokens, marian_tokens, spacy_spans=None, marian_spans=None):\n",
    "        if spacy_spans is None: spacy_spans = self.get_spans(spacy_tokens) \n",
    "        if marian_tokens is None: marian_spans = self.get_spans(marian_tokens) \n",
    "        \n",
    "        alignment = {} # map marian_tokens[i] to spacy_tokens[j]\n",
    "        best_overlap = defaultdict(int) # track max overlap\n",
    "        \n",
    "        # just bruteforce check (who needs DP?)\n",
    "        for i, marian_span in enumerate(marian_spans):\n",
    "            for j, spacy_span in enumerate(spacy_spans):\n",
    "                overlap = max(0, min(spacy_span[1], marian_span[1]) - max(spacy_span[0], marian_span[0]))\n",
    "                if overlap > 0 and overlap > best_overlap[i]:\n",
    "                    alignment[i] = j\n",
    "                    best_overlap[i] = overlap\n",
    "                    \n",
    "        for i, tok in enumerate(marian_tokens):\n",
    "            if tok == '': alignment[i] = alignment[i+1] # word break\n",
    "            if tok == '<unk>': alignment[i] = 1 + alignment[i-1] # unk\n",
    "        \n",
    "        assert len(alignment) == len(marian_tokens), f'did not find a complete alignment for: {text}'\n",
    "        return alignment\n",
    "\n",
    "    def get_spacy_tokens(self, text, lang):\n",
    "        return [token.text for token in (self.en_nlp if lang == 'en' else self.de_nlp)(text)]\n",
    "\n",
    "    def get_marian_tokens(self, text):\n",
    "        return [self.marian.decode(id) for id in self.marian(text)['input_ids'][:-1]]\n",
    "    \n",
    "    def align(self, text, lang):\n",
    "        spacy_tokens = self.get_spacy_tokens(text, lang)\n",
    "        spacy_spans = self.get_spans(text, spacy_tokens)\n",
    "        normalized_marian_tokens = self.get_marian_tokens(text)\n",
    "        marian_spans = self.get_spans(text, normalized_marian_tokens)\n",
    "\n",
    "        raw_marian_tokens = self.marian.tokenize(text)\n",
    "        doc = (self.en_nlp if lang == 'en' else self.de_nlp)(text)\n",
    "        alignment = self.get_alignment(text, spacy_tokens, normalized_marian_tokens, spacy_spans, marian_spans)\n",
    "    \n",
    "        # reverse map spacy to marian tokens\n",
    "        reverse = defaultdict(list)\n",
    "        for k, v in alignment.items(): reverse[v].append(k)\n",
    "\n",
    "        # get original root idx\n",
    "        root_i = next(i for i, tok in enumerate(doc) if tok.head == tok)\n",
    "\n",
    "        seq = []\n",
    "        for i, text in enumerate(raw_marian_tokens):\n",
    "            spacy_tok = doc[alignment[i]]\n",
    "            seq.append({'text': text, 'pos': spacy_tok.pos_, 'i': i, 'is_head': alignment[i] == root_i})\n",
    "        \n",
    "        for i, _ in enumerate(seq):\n",
    "            spacy_children = doc[alignment[i]].children\n",
    "            seq[i]['children'] = []\n",
    "            for child in spacy_children:\n",
    "                seq[i]['children'].extend(seq[i] for i in reverse[child.i])\n",
    "            \n",
    "            ### heuristic for lineage \n",
    "            # spacy_parent = doc[alignment[i]].head.i\n",
    "            # inferred_parent = reverse[spacy_parent][0]\n",
    "            seq[i]['par'] = reverse[doc[alignment[i]].head.i][0]\n",
    "            \n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import WMTForEvolver\n",
    "\n",
    "parser = Parser()\n",
    "wmt = WMTForEvolver(split='validation')\n",
    "\n",
    "traj = wmt._get_short_input_traj(parser.align(text, 'en'))\n",
    "\n",
    "for thing in traj:\n",
    "    print(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁The <- ▁restrictions\n",
      "▁new <- ▁restrictions\n",
      "▁restrictions <- ▁affect\n",
      "▁ <- ▁affect\n",
      "disproportionate <- ▁affect\n",
      "ly <- ▁affect\n",
      "▁affect <- ▁affect\n",
      "▁young <- ▁people\n",
      "▁people <- ▁affect\n",
      ", <- ▁people\n",
      "▁minorities <- ▁people\n",
      "▁and <- ▁minorities\n",
      "▁people <- ▁minorities\n",
      "▁with <- ▁affect\n",
      "▁low <- ▁incomes\n",
      "▁incomes <- ▁with\n",
      ". <- ▁affect\n"
     ]
    }
   ],
   "source": [
    "text = 'The new restrictions disproportionately affect young people, minorities and people with low incomes.'\n",
    "\n",
    "seq = parser.align(text, 'en')\n",
    "for thing in seq:\n",
    "    print(thing['text'], '<-', seq[thing['par']]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The <- restrictions\n",
      "new <- restrictions\n",
      "restrictions <- affect\n",
      "disproportionately <- affect\n",
      "affect <- affect\n",
      "young <- people\n",
      "people <- affect\n",
      ", <- people\n",
      "minorities <- people\n",
      "and <- minorities\n",
      "people <- minorities\n",
      "with <- affect\n",
      "low <- incomes\n",
      "incomes <- with\n",
      ". <- affect\n"
     ]
    }
   ],
   "source": [
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "for token in en_nlp(text):\n",
    "    # print({'text': token.text, 'pos': token.pos_, 'i': token.i, 'is_head': token == token.head})\n",
    "    print(token.text, '<-', token.head.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
