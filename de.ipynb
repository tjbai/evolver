{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sanity check overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from de import DependencyEvolver\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = DependencyEvolver(\n",
    "    d_model=16,\n",
    "    dim_feedforward=8,\n",
    "    nhead=1,\n",
    "    dropout=0,\n",
    "    N=5,\n",
    "    encoder_layers=1,\n",
    "    decoder_layers=1,\n",
    "    tok_v=tokenizer.vocab_size,\n",
    "    rel_v=2,\n",
    "    pos_v=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "batch = (\n",
    "    (8823, 2),\n",
    "    [\n",
    "        torch.tensor([[[1, 0, 2, 0, 3]]]),\n",
    "        torch.tensor([[[-1, -1, 1, -1, -1]]]),\n",
    "        torch.tensor([[[-1, 2, -1, 2, -1]]]),\n",
    "        torch.tensor([[[-1, 0, -1, 1, -1]]]),\n",
    "        torch.tensor([[[-1, 0, -1, 1, -1]]]),\n",
    "        torch.tensor([[[-1, 2016, -1, 2833, -1]]])\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_loader = [batch for _ in range(1000)]\n",
    "eval_loader = [batch for _ in range(20)]\n",
    "optim = AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._train(optim, train_loader, eval_loader, len(train_loader), 20, 1, 1e9, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## full data creation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "\n",
    "with open('./data/ud/en_gum-ud-dev.conllu', 'r') as f:\n",
    "    sentences = conllu.parse(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct adjacent sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def family_tree(parsed):\n",
    "    children = {tok['id']: [] for tok in parsed}\n",
    "    for tok in parsed:\n",
    "        if tok['head'] is None: return None\n",
    "        if tok['head'] != 0: children[tok['head']].append(tok['id'])\n",
    "        \n",
    "    i, root = next((i, tok) for (i, tok) in enumerate(parsed) if tok['head'] == 0)\n",
    "\n",
    "    seqs = [[(root['form'], root['upos'], root['deprel'], i, True, -1)]]\n",
    "    cur_leaves = [root['id']]\n",
    "    all_leaves = [root['id']]\n",
    "\n",
    "    while cur_leaves:\n",
    "        seq = []\n",
    "        next_leaves = []\n",
    "        \n",
    "        for i, tok in enumerate(parsed):\n",
    "            if tok['id'] in all_leaves or tok['head'] in all_leaves:\n",
    "                seq.append((\n",
    "                    tok['form'], tok['upos'], tok['deprel'],\n",
    "                    i, tok['head'] in cur_leaves, None\n",
    "                ))\n",
    "                if tok['head'] in cur_leaves: next_leaves.extend(children[tok['id']])\n",
    "                \n",
    "        for i, (form, upos, deprel, j, is_leaf, _) in enumerate(seq):\n",
    "            tok = next(t for t in parsed if t['form'] == form and t['upos'] == upos and t['deprel'] == deprel)\n",
    "            if tok['head'] == 0:\n",
    "                par = -1\n",
    "            else:\n",
    "                par = next((j for j, (p_form, p_upos, p_deprel, _, _, _) in enumerate(seq)\n",
    "                            if p_form == parsed[tok['head']-1]['form']\n",
    "                            and p_upos == parsed[tok['head']-1]['upos']\n",
    "                            and p_deprel == parsed[tok['head']-1]['deprel']),\n",
    "                            None)\n",
    "            \n",
    "            seq[i] = (form, upos, deprel, j, is_leaf, par)\n",
    "        \n",
    "        seqs.append(seq)\n",
    "        cur_leaves = next_leaves\n",
    "        all_leaves.extend(next_leaves)\n",
    "        \n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_map = {}\n",
    "rel_map = {}\n",
    "pos_map = {}\n",
    "\n",
    "with open('vocab/gum_tok.vocab', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        tok_map[line.strip()] = i\n",
    "\n",
    "with open('vocab/rel.vocab', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        rel_map[line.strip()] = len(tok_map) + i\n",
    "        \n",
    "with open('vocab/pos.vocab', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        pos_map[line.strip()] = len(tok_map) + len(rel_map) + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17755 69 17\n"
     ]
    }
   ],
   "source": [
    "print(len(tok_map), len(rel_map), len(pos_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from de import INS_ID, CPY_ID, PRO_ID, EOS_ID\n",
    "\n",
    "def label(seqs, N):\n",
    "    traj = [[] for _ in range(6)]\n",
    "\n",
    "    for i in range(1, len(seqs)-1):\n",
    "        a, b = seqs[i], seqs[i+1]\n",
    "    \n",
    "        op_list = [-1]\n",
    "        cpy_list = [-1]\n",
    "        par_list = [-1]\n",
    "        tok_list = [-1]\n",
    "        pos_list = [-1]\n",
    "        rel_list = [-1]\n",
    "        \n",
    "        prev = {i: is_leaf for (_, _, _, i, is_leaf, _) in a}\n",
    "        for i, (tok, pos, rel, j, _, par) in enumerate(b[:N-2]):\n",
    "            if j in prev:\n",
    "                op_list.append(PRO_ID if prev[j] else CPY_ID)\n",
    "                cpy_list.append(next(i for i, t in enumerate(a) if t[3] == j) + 1)\n",
    "                par_list.append(-1)\n",
    "                tok_list.append(-1)\n",
    "                rel_list.append(-1)\n",
    "                pos_list.append(-1)\n",
    "            else:\n",
    "                op_list.append(INS_ID)\n",
    "                cpy_list.append(-1)\n",
    "                par_list.append(par + 1)\n",
    "                tok_list.append(tok_map.get(tok, len(tok_map) + len(rel_map) + len(pos_map)))\n",
    "                rel_list.append(rel_map[rel])\n",
    "                pos_list.append(pos_map[pos])\n",
    "                \n",
    "        op_list.append(EOS_ID)\n",
    "        for i, list in enumerate([op_list, cpy_list, par_list, tok_list, pos_list, rel_list]):\n",
    "            list.extend([-1 for _ in range(N-len(list))])\n",
    "            traj[i].append(list)\n",
    "            \n",
    "    return traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "traj_list = []\n",
    "\n",
    "type1 = 0\n",
    "type2 = 0\n",
    "\n",
    "for parsed in tqdm(sentences):\n",
    "    seqs = family_tree(parsed)\n",
    "    if seqs is None:\n",
    "        type1 += 1\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        traj_list.append(label(seqs, 64))\n",
    "    except TypeError as e:\n",
    "        type2 += 1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/gum/dev_1.pkl', 'wb') as f:\n",
    "    pickle.dump(traj_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
