{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "parse = nlp('Reform number one, of course, is to ensure that the next World Bank President is not an American.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {s: i for i, s in enumerate(['BOS', 'EOS'] + list(nlp.vocab.strings) + list(spacy.glossary.GLOSSARY))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth(doc):\n",
    "    root = [tok for tok in doc if tok.head == tok] [0]\n",
    "    def dfs(node):\n",
    "        r = 1\n",
    "        for child in node.children: r = max(r, 1 + dfs(child))\n",
    "        return r\n",
    "    return dfs(root)\n",
    "\n",
    "depth(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(doc):\n",
    "    INS, CPY, SUB = 0, 1, 2 \n",
    "    \n",
    "    traj = [['_' for _ in range(len(doc))] for _ in range(2*depth(doc))]\n",
    "    \n",
    "    def traverse(token, depth):\n",
    "        for i in range(depth, len(traj)):\n",
    "            traj[i][token.i] = (token.text if (i > depth+1) else token.pos_, token.i, token.head.i)\n",
    "        \n",
    "        traj[depth+1][token.i] = (token.text, token.i, token.head.i)\n",
    "        \n",
    "        for child in token.children:\n",
    "            traverse(child, depth+2)\n",
    "    \n",
    "    root = next(token for token in doc if token.head == token)\n",
    "    traverse(root, 0)\n",
    "   \n",
    "    res = [[root.pos_]]\n",
    "    edit_traj = [[(INS, 'BOS', -1), (INS, root.text, -1), (INS, 'EOS', -1)]]\n",
    "    \n",
    "    # for i, seq in enumerate(traj[1:]):\n",
    "    #     cur_edits = [(CPY, -1, 0)]\n",
    "           \n",
    "    #     if i % 2 == 0:\n",
    "    #         k = 1\n",
    "    #         for t in seq:\n",
    "    #             if t == '_': continue\n",
    "    #             if t[1] in m:\n",
    "    #                 cur_edits.append((CPY, -1, k))\n",
    "    #             else:\n",
    "    #                 cur_edits.append((SUB, t[0], k))\n",
    "    #             m[t[1]] = k\n",
    "    #             k += 1\n",
    "                \n",
    "    #     else:\n",
    "    #         k = 1\n",
    "    #         for t in seq:\n",
    "    #             if t == '_': continue\n",
    "    #             if t[1] in m:\n",
    "    #                 cur_edits.append((CPY, -1, m[t[1]]))\n",
    "    #             else:\n",
    "    #                 cur_edits.append((SUB, t[0], m[t[2]]))\n",
    "           \n",
    "    #     res.append([t[0] for t in seq if t != '_'])\n",
    "    #     cur_edits.append((CPY, -1, len(edit_traj[-1])+1))\n",
    "    #     edit_traj.append(cur_edits)\n",
    "    \n",
    "    m = {root.i: 1}\n",
    "    for i, seq in enumerate(traj[3::2]):\n",
    "        cur_edits = [(CPY, -1, 0)]\n",
    "        for t in seq:\n",
    "            k = 1\n",
    "            if t == '_': continue\n",
    "            if t[1] in m:\n",
    "                cur_edits.append((CPY, -1, m[t[1]]))\n",
    "            else:\n",
    "                cur_edits.append((SUB, t[0], m[t[2]]))\n",
    "            m[t[1]] = k\n",
    "            k += 1\n",
    "       \n",
    "        cur_edits.append((CPY, -1, len(edit_traj[-1])+1)) \n",
    "        edit_traj.append(cur_edits)\n",
    "    \n",
    "    return res, edit_traj\n",
    "\n",
    "traj, edit_traj = gen(parse)\n",
    "\n",
    "for t in traj:\n",
    "    print(' '.join(t))\n",
    "\n",
    "print()    \n",
    "for e in edit_traj:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## figure out alignment!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
    "ids = tokenizer.encode(str(parse))\n",
    "tokenizer.decode(ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens = [str(t).lower() for t in parse]\n",
    "\n",
    "marian_tokens = [\n",
    "    t[1:].lower() if ord(t[0]) == 9601 else t.lower()\n",
    "    for t in tokenizer.tokenize(str(parse))\n",
    "]\n",
    "\n",
    "# we can check and remove if the first character is ord = 9601\n",
    "\n",
    "print(spacy_tokens)\n",
    "print(marian_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = load_dataset('wmt14', 'de-en', split='test')\n",
    "\n",
    "with open('tmp', 'w') as f:\n",
    "    for pair in tqdm(dataset['translation']):\n",
    "        en = pair['en']\n",
    "        parse = nlp(en)\n",
    "        \n",
    "        spacy_tokens = [str(t).lower() for t in parse]\n",
    "        \n",
    "        marian_tokens = [\n",
    "            t[1:].lower() if ord(t[0]) == 9601 else t.lower()\n",
    "            for t in tokenizer.tokenize(str(parse))]\n",
    "       \n",
    "        f.write(json.dumps(spacy_tokens))\n",
    "        f.write('\\n')\n",
    "        f.write(json.dumps(marian_tokens))\n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mt.py --config=./configs/wmt/ar-toy.json --local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mt.py --config=./configs/wmt/evolver-toy.json --skip --local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mt.py --config=./configs/wmt/teacher-toy.json --local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build static vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "de_nlp = spacy.load('de_core_news_sm')\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "en_toks = set()\n",
    "en_pos = set()\n",
    "de_toks = set()\n",
    "de_pos = set()\n",
    "\n",
    "dataset = load_dataset('wmt14', 'de-en')\n",
    "for split in ['train', 'test', 'validation']:\n",
    "    for pair in tqdm(dataset[split]['translation'], desc=f'crawling {split}'):\n",
    "        for de in de_nlp(pair['de']):\n",
    "            de_toks.add(de.text)\n",
    "            de_pos.add(de.pos_)\n",
    "        for en in en_nlp(pair['de']):\n",
    "            en_toks.add(en.text) \n",
    "            en_pos.add(en.pos_) \n",
    "    \n",
    "    print(f'crawled {len(en_toks)} en_toks, {len(en_pos)} en_pos, {len(de_toks)} de_toks, {len(de_pos)} de_pos')\n",
    "    vocab = en_toks.union(en_pos, de_toks, de_pos)\n",
    "    with open('vocab/wmt14_de_en.vocab', 'w') as f:\n",
    "        for v in tqdm(vocab, desc=f'dumping {split}'):\n",
    "            f.write(v)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq edit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTEditDataset\n",
    "\n",
    "dataset = MTEditDataset(split='test', max_len=128, buffer_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ids, input_ids, edit_ids = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mt import SpacyTokenizer\n",
    "\n",
    "tok = SpacyTokenizer()\n",
    "\n",
    "print(input_ids)\n",
    "print(edit_ids)\n",
    "\n",
    "print(tok.decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tok = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTDataset, BertTokenizer\n",
    "\n",
    "dataset = MTDataset(split='test', tokenizer=BertTokenizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debug transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MarianTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = MarianTokenizer()\n",
    "\n",
    "dataset = load_dataset('wmt14', 'de-en', split='test')\n",
    "\n",
    "len_de = []\n",
    "len_en = []\n",
    "\n",
    "for thing in tqdm(dataset['translation']):\n",
    "    de = thing['de']\n",
    "    en = thing['en']\n",
    "    len_de.append(len(tokenizer.encode(de)))\n",
    "    len_en.append(len(tokenizer.encode(en)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(x):\n",
    "    x['translation']  = x['translation'][:4]\n",
    "\n",
    "dataset.map(truncate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import TrajectoryDataset, Teacher, SpacyTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = SpacyTokenizer()\n",
    "\n",
    "dataset = TrajectoryDataset(split='test', truncate=4, tokenizer=tokenizer)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, collate_fn=dataset.collate_fn)\n",
    "\n",
    "teacher = Teacher(\n",
    "    d_model=64,\n",
    "    dim_feedforward=256,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_len=20)\n",
    "\n",
    "batch = next(iter(loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "for thing in islice(loader, 10):\n",
    "    print(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = teacher.rollout({'src_ids': batch['src_ids'], 'root_ids': batch['root_ids']}, T=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debug evolver toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "truncated dataset:\n",
    "\n",
    "```json\n",
    "{\n",
    " \"paragraphs\": [\n",
    "   {\n",
    "     \"de\": \"Wiederaufnahme der Sitzungsperiode\",\n",
    "     \"en\": \"Resumption of the session\"\n",
    "   },\n",
    "   {\n",
    "     \"de\": \"Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.\",\n",
    "     \"en\": \"I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\"\n",
    "   },\n",
    "   {\n",
    "     \"de\": \"Wie Sie feststellen konnten, ist der gefürchtete \\\"Millenium-Bug\\\" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.\",\n",
    "     \"en\": \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"\n",
    "   },\n",
    "   {\n",
    "     \"de\": \"Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen.\",\n",
    "     \"en\": \"You have requested a debate on this subject in the course of the next few days, during this part-session.\"\n",
    "   }\n",
    " ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mt import SpacyTokenizer, TrajectoryDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = SpacyTokenizer()\n",
    "dataset = TrajectoryDataset(split='train', truncate=4, tokenizer=tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=1, collate_fn=dataset.collate_fn, shuffle=False)\n",
    "\n",
    "batch = next(iter(loader))\n",
    "\n",
    "tokenizer.decode(batch['src_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mt import Evolver, Teacher1\n",
    "\n",
    "config = {\n",
    "    'd_model': 64,\n",
    "    'dim_feedforward': 256,\n",
    "    'nhead': 4,\n",
    "    'dropout': 0.1,\n",
    "    'layer_norm_eps': 1e-5,\n",
    "    'decoder_layers': 4,\n",
    "    'encoder_layers': 4,\n",
    "    'max_len': 128,\n",
    "    'bos_token_id': tokenizer.bos_token_id,\n",
    "    'eos_token_id': tokenizer.eos_token_id,\n",
    "    'pad_token_id': tokenizer.pad_token_id,\n",
    "    'vocab_size': tokenizer.vocab_size,\n",
    "    'name': None\n",
    "}\n",
    "\n",
    "evolver = Evolver(**config)\n",
    "teacher = Teacher(**config)\n",
    "\n",
    "evolver.load_state_dict(torch.load('mt_evolver_64d_4enc_4dec-20241021_200919_10000.pt', map_location='cpu')['model'])\n",
    "teacher.load_state_dict(torch.load('mt_teacher_64d_4enc_4dec-20241021_200959_10000.pt', map_location='cpu')['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_ids = dataset[2]['traj_ids']\n",
    "\n",
    "for step in traj_ids:\n",
    "    print(tokenizer.decode(step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolver.eval()\n",
    "\n",
    "traj = evolver.rollout(batch, T=5, temp=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in traj:\n",
    "    print(tokenizer.decode(step[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher.eval()\n",
    "\n",
    "traj = teacher.rollout(batch, T=5, verbose=True)\n",
    "\n",
    "for step in traj:\n",
    "    print(tokenizer.decode(step[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import evaluate\n",
    "\n",
    "evaluate(teacher, loader, 'cpu', 2, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### observation: errors compound and we fall off the rails\n",
    "\n",
    "idea 1: retrain with a simpler inductive bias\n",
    "\n",
    "idea 2: can we tweak temperature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = evolver.rollout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "bert_tok = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence = \"here's a sentence with possibly more complicated tokenization.\"\n",
    "\n",
    "parse = nlp(sentence)\n",
    "\n",
    "# print(''.join(tok.text_with_ws for tok in parse))\n",
    "\n",
    "bert_tok.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# refresh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import WMT\n",
    "\n",
    "wmt = WMT(split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import SpacyTokenizer\n",
    "\n",
    "tokenizer = SpacyTokenizer()\n",
    "\n",
    "tokenizer.decode(wmt[0]['src_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import WMTForEvolver\n",
    "\n",
    "dataset = WMTForEvolver(split='validation')\n",
    "\n",
    "input = 'Hello my name is John Doe.'\n",
    "\n",
    "doc = tokenizer.en_nlp(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = dataset.gen(doc)\n",
    "\n",
    "for thing in traj:\n",
    "    print(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset._get_output_traj(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
