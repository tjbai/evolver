{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import spacy\n",
    "\n",
    "dataset = datasets.load_dataset(\"wmt14\", \"cs-en\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'The world needs the World Bank a lot more than it needs another condominium.'\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "parse = nlp(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {s: i for i, s in enumerate(['BOS', 'EOS'] + list(nlp.vocab.strings) + list(spacy.glossary.GLOSSARY))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth(doc):\n",
    "    root = [tok for tok in doc if tok.head == tok] [0]\n",
    "    def dfs(node):\n",
    "        r = 1\n",
    "        for child in node.children: r = max(r, 1 + dfs(child))\n",
    "        return r\n",
    "    return dfs(root)\n",
    "\n",
    "depth(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(doc):\n",
    "    INS, CPY, SUB = 0, 1, 2 \n",
    "    \n",
    "    traj = [['_' for _ in range(len(doc))] for _ in range(2*depth(doc))]\n",
    "    \n",
    "    def traverse(token, depth):\n",
    "        for i in range(depth, len(traj)):\n",
    "            traj[i][token.i] = (token.text if (i > depth+1) else token.pos_, token.i, token.head.i)\n",
    "        \n",
    "        traj[depth+1][token.i] = (token.text, token.i, token.head.i)\n",
    "        \n",
    "        for child in token.children:\n",
    "            traverse(child, depth+2)\n",
    "    \n",
    "    root = next(token for token in doc if token.head == token)\n",
    "    traverse(root, 0)\n",
    "    \n",
    "    edit_traj = [[(INS, 'BOS', -1), (INS, root.pos_, -1), (INS, 'EOS', -1)]]\n",
    "    m = {}\n",
    "    \n",
    "    for i, seq in enumerate(traj[1:]):\n",
    "        cur_edits = [(CPY, -1, 0)]\n",
    "           \n",
    "        if i % 2 == 0:\n",
    "            k = 1\n",
    "            for t in seq:\n",
    "                if t == '_': continue\n",
    "                if t[1] in m:\n",
    "                    cur_edits.append((CPY, -1, k))\n",
    "                else: \n",
    "                    cur_edits.append((SUB, t[0], k))\n",
    "                m[t[1]] = k\n",
    "                k += 1\n",
    "                \n",
    "        else:\n",
    "            k = 1\n",
    "            for t in seq:\n",
    "                if t == '_': continue\n",
    "                if t[1] in m:\n",
    "                    cur_edits.append((CPY, -1, m[t[1]]))\n",
    "                else:\n",
    "                    cur_edits.append((SUB, t[0], m[t[2]]))\n",
    "                    \n",
    "        cur_edits.append((CPY, -1, len(edit_traj[-1])+1))\n",
    "        edit_traj.append(cur_edits)\n",
    "    \n",
    "    return traj, edit_traj\n",
    "\n",
    "traj, edit_traj = gen(parse)\n",
    "\n",
    "for t in traj:\n",
    "    print(' '.join(a[0] for a in t))\n",
    "\n",
    "print()    \n",
    "for e in edit_traj:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import MTDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = MTDataset(split='train', max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab/wmt14_de_en.vocab', 'r') as f:\n",
    "    print(len(f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = max(v for _, v in tokenizer.vocab.items())\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = load_dataset('wmt14', 'de-en')\n",
    "\n",
    "de_len = []\n",
    "en_len = []\n",
    "\n",
    "for pair in tqdm(dataset['train']['translation'][:5000]):\n",
    "    de_len.append(len(tokenizer.encode(pair['de'], lang='de')))\n",
    "    en_len.append(len(tokenizer.encode(pair['en'], lang='en')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(de_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(en_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mt.py --config=./configs/wmt/ar-toy.json --local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build static vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "de_nlp = spacy.load('de_core_news_sm')\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "en_toks = set()\n",
    "en_pos = set()\n",
    "de_toks = set()\n",
    "de_pos = set()\n",
    "\n",
    "dataset = load_dataset('wmt14', 'de-en')\n",
    "for split in ['train', 'test', 'validation']:\n",
    "    for pair in tqdm(dataset[split]['translation'], desc=f'crawling {split}'):\n",
    "        for de in de_nlp(pair['de']):\n",
    "            de_toks.add(de.text)\n",
    "            de_pos.add(de.pos_)\n",
    "        for en in en_nlp(pair['de']):\n",
    "            en_toks.add(en.text) \n",
    "            en_pos.add(en.pos_) \n",
    "    \n",
    "    print(f'crawled {len(en_toks)} en_toks, {len(en_pos)} en_pos, {len(de_toks)} de_toks, {len(de_pos)} de_pos')\n",
    "    vocab = en_toks.union(en_pos, de_toks, de_pos)\n",
    "    with open('vocab/wmt14_de_en.vocab', 'w') as f:\n",
    "        for v in tqdm(vocab, desc=f'dumping {split}'):\n",
    "            f.write(v)\n",
    "            f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
