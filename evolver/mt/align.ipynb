{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from transformers import MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to make sure the character-based span matching will _actually_ match correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spans(text, tokens):\n",
    "    spans = []\n",
    "    i = 0\n",
    "    for token in tokens:\n",
    "        while i < len(text) and text[i].isspace(): i += 1\n",
    "        if i < len(text):\n",
    "            s = i\n",
    "            i = text.find(token, i) + len(token)\n",
    "            spans.append((s, i))\n",
    "    \n",
    "    assert len(spans) == len(tokens), f'number of spans does not match number of tokens for: {text}'\n",
    "    return spans\n",
    "\n",
    "def get_alignments(text, spacy_tokens, marian_tokens):\n",
    "    spacy_spans = get_spans(text, spacy_tokens)\n",
    "    marian_spans = get_spans(text, marian_tokens)\n",
    "    alignment = {} # map marian_tokens[i] to spacy_tokens[j]\n",
    "    best_overlap = defaultdict(int) # track max overlap\n",
    "    \n",
    "    # just bruteforce check (who needs DP?)\n",
    "    for i, marian_span in enumerate(marian_spans):\n",
    "        for j, spacy_span in enumerate(spacy_spans):\n",
    "            overlap = max(0, min(spacy_span[1], marian_span[1]) - max(spacy_span[0], marian_span[0]))\n",
    "            if overlap > 0 and overlap > best_overlap[i]:\n",
    "                alignment[i] = j\n",
    "                best_overlap[i] = overlap\n",
    "                \n",
    "    for i, tok in enumerate(marian_tokens):\n",
    "        if tok == '': alignment[i] = alignment[i+1] # word break\n",
    "        if tok == '<unk>': alignment[i] = 1 + alignment[i-1] # unk\n",
    "    \n",
    "    assert len(alignment) == len(marian_tokens), f'did not find a complete alignment for: {text}'\n",
    "    return alignment\n",
    "\n",
    "marian = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "wmt = load_dataset('wmt14', 'de-en', split='validation')\n",
    "\n",
    "def get_spacy_tokens(text):\n",
    "    return [token.text for token in nlp(text)]\n",
    "\n",
    "def get_marian_tokens(text):\n",
    "    return [marian.decode(id) for id in marian(text)['input_ids'][:-1]]\n",
    "\n",
    "for i, example in enumerate(tqdm(wmt['translation'])):\n",
    "    # text = example['en']\n",
    "    text = example['de']\n",
    "    spacy_tokens = get_spacy_tokens(text)\n",
    "    marian_tokens = get_marian_tokens(text)\n",
    "    alignments = get_alignments(text, spacy_tokens, marian_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.en_nlp = spacy.load('en_core_web_sm')\n",
    "        self.de_nlp = spacy.load('de_core_news_sm')\n",
    "        self.marian = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
    "    \n",
    "    def get_spans(self, text, tokens):\n",
    "        spans = []\n",
    "        i = 0\n",
    "        for token in tokens:\n",
    "            while i < len(text) and text[i].isspace(): i += 1\n",
    "            if i < len(text):\n",
    "                s = i\n",
    "                i = text.find(token, i) + len(token)\n",
    "                spans.append((s, i))\n",
    "        \n",
    "        assert len(spans) == len(tokens), f'number of spans does not match number of tokens for: {text}'\n",
    "        return spans\n",
    "\n",
    "    def get_alignment(self, text, spacy_tokens, marian_tokens, spacy_spans=None, marian_spans=None):\n",
    "        if spacy_spans is None: spacy_spans = self.get_spans(spacy_tokens) \n",
    "        if marian_tokens is None: marian_spans = self.get_spans(marian_tokens) \n",
    "        \n",
    "        alignment = {} # map marian_tokens[i] to spacy_tokens[j]\n",
    "        best_overlap = defaultdict(int) # track max overlap\n",
    "        \n",
    "        # just bruteforce check (who needs DP?)\n",
    "        for i, marian_span in enumerate(marian_spans):\n",
    "            for j, spacy_span in enumerate(spacy_spans):\n",
    "                overlap = max(0, min(spacy_span[1], marian_span[1]) - max(spacy_span[0], marian_span[0]))\n",
    "                if overlap > 0 and overlap > best_overlap[i]:\n",
    "                    alignment[i] = j\n",
    "                    best_overlap[i] = overlap\n",
    "                    \n",
    "        for i, tok in enumerate(marian_tokens):\n",
    "            if tok == '': alignment[i] = alignment[i+1] # word break\n",
    "            if tok == '<unk>': alignment[i] = 1 + alignment[i-1] # unk\n",
    "        \n",
    "        assert len(alignment) == len(marian_tokens), f'did not find a complete alignment for: {text}'\n",
    "        return alignment\n",
    "\n",
    "    def get_spacy_tokens(self, text, lang):\n",
    "        return [token.text for token in (self.en_nlp if lang == 'en' else self.de_nlp)(text)]\n",
    "\n",
    "    def get_marian_tokens(self, text):\n",
    "        return [self.marian.decode(id) for id in self.marian(text)['input_ids'][:-1]]\n",
    "    \n",
    "    def align(self, text, lang):\n",
    "        spacy_tokens = self.get_spacy_tokens(text, lang)\n",
    "        spacy_spans = self.get_spans(text, spacy_tokens)\n",
    "        normalized_marian_tokens = self.get_marian_tokens(text)\n",
    "        marian_spans = self.get_spans(text, normalized_marian_tokens)\n",
    "\n",
    "        raw_marian_tokens = self.marian.tokenize(text)\n",
    "        doc = (self.en_nlp if lang == 'en' else self.de_nlp)(text)\n",
    "        alignment = self.get_alignment(text, spacy_tokens, normalized_marian_tokens, spacy_spans, marian_spans)\n",
    "    \n",
    "        # reverse map spacy to marian tokens\n",
    "        reverse = defaultdict(list)\n",
    "        for k, v in alignment.items(): reverse[v].append(k)\n",
    "\n",
    "        # get original root idx\n",
    "        root_i = next(i for i, tok in enumerate(doc) if tok.head == tok)\n",
    "\n",
    "        seq = []\n",
    "        for i, text in enumerate(raw_marian_tokens):\n",
    "            spacy_tok = doc[alignment[i]]\n",
    "            seq.append({'text': text, 'pos': spacy_tok.pos_, 'i': i, 'is_head': alignment[i] == root_i})\n",
    "        \n",
    "        for i, _ in enumerate(seq):\n",
    "            spacy_children = doc[alignment[i]].children\n",
    "            seq[i]['children'] = []\n",
    "            for child in spacy_children:\n",
    "                seq[i]['children'].extend(seq[i] for i in reverse[child.i])\n",
    "            \n",
    "            ### heuristic for lineage \n",
    "            # spacy_parent = doc[alignment[i]].head.i\n",
    "            # inferred_parent = reverse[spacy_parent][0]\n",
    "            seq[i]['par'] = reverse[doc[alignment[i]].head.i][0]\n",
    "            \n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import WMTForEvolver\n",
    "\n",
    "parser = Parser()\n",
    "wmt = WMTForEvolver(split='validation')\n",
    "\n",
    "traj = wmt._get_short_input_traj(parser.align(text, 'en'))\n",
    "\n",
    "for thing in traj:\n",
    "    print(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The new restrictions disproportionately affect young people, minorities and people with low incomes.'\n",
    "\n",
    "seq = parser.align(text, 'en')\n",
    "for thing in seq:\n",
    "    print(thing['text'], '<-', seq[thing['par']]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "for token in en_nlp(text):\n",
    "    # print({'text': token.text, 'pos': token.pos_, 'i': token.i, 'is_head': token == token.head})\n",
    "    print(token.text, '<-', token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test loader ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt import WMTForEvolver\n",
    "\n",
    "dataset = WMTForEvolver(split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test that everything we load actually applies to create the correct output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def apply_edits(input_ids, edit_ids):\n",
    "    op_ids, tok_ids, idx_ids = edit_ids\n",
    "    res = torch.zeros(op_ids.shape[1], dtype=torch.long)\n",
    "    ins_mask = op_ids.eq(0) | op_ids.eq(2)\n",
    "    res[ins_mask] = tok_ids[ins_mask]\n",
    "    cpy_mask = op_ids.eq(1)\n",
    "    permuted_inputs = input_ids[torch.arange(1).view(-1, 1), idx_ids]\n",
    "    res[cpy_mask] = permuted_inputs[cpy_mask]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "debug index error in parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import MarianTokenizer\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "bad_apples = 0\n",
    "\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    try: _ = dataset[i]\n",
    "    except IndexError:\n",
    "        print(dataset.dataset[i]['translation']['en'])\n",
    "        bad_apples += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Parser\n",
    "\n",
    "parser = Parser()\n",
    "\n",
    "text = 'In fact, 11% of American citizens, i.e. 21 million people of voting age, do not possess a photo ID card issued by a government agency of their State.'\n",
    "\n",
    "spacy_tokens = parser.get_spacy_tokens(text)\n",
    "print(spacy_tokens)\n",
    "\n",
    "spacy_spans = parser.get_spans(text, spacy_tokens)\n",
    "print(spacy_spans)\n",
    "\n",
    "marian_tokens = parser.get_marian_tokens(text)\n",
    "print(marian_tokens)\n",
    "\n",
    "marian_spans = parser.get_spans(text, marian_tokens)\n",
    "print(marian_spans)\n",
    "\n",
    "alignment = parser.get_alignment(text, spacy_tokens, marian_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about 5% of our data is jank because of bad alignments!!!! (not really, fixed with percentage edge cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "reverse = defaultdict(list)\n",
    "for k, v in alignment.items(): reverse[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tok in enumerate(seq):\n",
    "    spacy_par = doc[alignment[i]]\n",
    "    print(f'{i}: {tok[\"text\"]} -> {spacy_par.i}: {spacy_par.text}')\n",
    "    print(f'derives from {spacy_par.head.i}: {spacy_par.head.text}')\n",
    "    print(f'reverse: {reverse[spacy_par.head.i]}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
