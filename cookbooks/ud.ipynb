{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "\n",
    "with open('../data/ud/en_ewt-ud-train.conllu', 'r') as f:\n",
    "    sentences = conllu.parse(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xcomp', 'list', 'advcl:relcl', 'vocative', 'det:predet', 'dep', 'obl:tmod', 'iobj', 'advmod', 'appos', 'nmod:tmod', 'acl:relcl', 'nmod', 'case', '_', 'conj', 'acl', 'goeswith', 'nsubj', 'ccomp', 'nmod:npmod', 'root', 'nmod:desc', 'obl:npmod', 'cop', 'parataxis', 'aux', 'csubj', 'csubj:pass', 'det', 'compound:prt', 'discourse', 'mark', 'expl', 'advcl', 'cc', 'reparandum', 'csubj:outer', 'nummod', 'nsubj:pass', 'cc:preconj', 'dislocated', 'fixed', 'aux:pass', 'obl:agent', 'nsubj:outer', 'amod', 'obj', 'nmod:poss', 'orphan', 'obl', 'punct', 'compound', 'flat'}\n"
     ]
    }
   ],
   "source": [
    "deps = set()\n",
    "\n",
    "for sent in sentences:\n",
    "    for tok in sent:\n",
    "        deps.add(tok['deprel'])\n",
    "        \n",
    "print(deps)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_',\n",
       " 'acl',\n",
       " 'acl:relcl',\n",
       " 'advcl',\n",
       " 'advcl:relcl',\n",
       " 'advmod',\n",
       " 'amod',\n",
       " 'appos',\n",
       " 'aux',\n",
       " 'aux:pass',\n",
       " 'case',\n",
       " 'cc',\n",
       " 'cc:preconj',\n",
       " 'ccomp',\n",
       " 'compound',\n",
       " 'compound:prt',\n",
       " 'conj',\n",
       " 'cop',\n",
       " 'csubj',\n",
       " 'csubj:outer',\n",
       " 'csubj:pass',\n",
       " 'dep',\n",
       " 'det',\n",
       " 'det:predet',\n",
       " 'discourse',\n",
       " 'dislocated',\n",
       " 'expl',\n",
       " 'fixed',\n",
       " 'flat',\n",
       " 'goeswith',\n",
       " 'iobj',\n",
       " 'list',\n",
       " 'mark',\n",
       " 'nmod',\n",
       " 'nmod:desc',\n",
       " 'nmod:npmod',\n",
       " 'nmod:poss',\n",
       " 'nmod:tmod',\n",
       " 'nsubj',\n",
       " 'nsubj:outer',\n",
       " 'nsubj:pass',\n",
       " 'nummod',\n",
       " 'obj',\n",
       " 'obl',\n",
       " 'obl:agent',\n",
       " 'obl:npmod',\n",
       " 'obl:tmod',\n",
       " 'orphan',\n",
       " 'parataxis',\n",
       " 'punct',\n",
       " 'reparandum',\n",
       " 'root',\n",
       " 'vocative',\n",
       " 'xcomp'}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy = {\n",
    "    'NOUN': ['NOUN', 'PROPN'],\n",
    "    'VERB': ['VERB', 'AUX'],\n",
    "    'MOD': ['ADJ', 'ADV'],\n",
    "    'FUNC': ['DET', 'PRON', 'ADP', 'CCONJ', 'SCONJ', 'PART'],\n",
    "    'NUM': ['NUM'],\n",
    "    'OTHER': ['INTJ', 'SYM', 'PUNCT', 'X', '_']\n",
    "}\n",
    "\n",
    "to_parent = {}\n",
    "for k, vs in hierarchy.items():\n",
    "    to_parent[k] = k\n",
    "    for v in vs:\n",
    "        to_parent[v] = k\n",
    "        \n",
    "to_parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_sentences = []\n",
    "for sent in sentences:\n",
    "    cur = []\n",
    "    for tok in sent:\n",
    "        cur.append((tok['form'], to_parent[tok['upostag']]))\n",
    "    m_sentences.append(cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detok = TreebankWordDetokenizer()\n",
    "\n",
    "def noise(sent, init=0.2):\n",
    "    traj = [detok.detokenize([s[0] for s in sent])]\n",
    "    \n",
    "    for drop in ['OTHER', 'MOD', 'FUNC', 'NUM', 'VERB', 'NOUN']:\n",
    "        new_sent = []\n",
    "        for tok, pos in sent:\n",
    "            if pos != drop: new_sent.append((tok, pos))\n",
    "        sent = new_sent\n",
    "        traj.append(detok.detokenize([s[0] for s in sent]))\n",
    "        \n",
    "    return traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = np.random.choice([1,2,3], 2, replace=False, p=[1/3,1/3,1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2, 3}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "importance = {\n",
    "    'OTHER': 1.5,\n",
    "    'FUNC': 2,\n",
    "    'NUM': 4,\n",
    "    'MOD': 5,\n",
    "    'VERB': 8,\n",
    "    'NOUN': 9\n",
    "}\n",
    "\n",
    "def noise(sent, traj_length=6):\n",
    "    traj = [detok.detokenize([s[0] for s in sent])]\n",
    "   \n",
    "    for i in range(traj_length):\n",
    "        if not sent: break\n",
    "        N = max(1, len(sent) // (traj_length - i))\n",
    "        \n",
    "        weights = [1 / importance[pos] for _, pos in sent]\n",
    "        tot = sum(weights)\n",
    "        weights = [w / tot for w in weights]\n",
    "        \n",
    "        to_drop = np.random.choice(\n",
    "            range(len(sent)),\n",
    "            min(N, len(sent)),\n",
    "            p=weights,\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        sent  = [tok for j, tok in enumerate(sent) if j not in to_drop]\n",
    "        traj.append(detok.detokenize(s[0] for s in sent))\n",
    "        \n",
    "    if traj[-1] != '': traj.append('')\n",
    "    return traj[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'killed member Party kidnapping',\n",
       " 'Zaman killed member Kurdistan Democratic Party kidnapping',\n",
       " 'Zaman Guerrillas killed member the Kurdistan Democratic Party kidnapping Mosul',\n",
       " 'Al Zaman Guerrillas killed member the Kurdistan Democratic Party after kidnapping Mosul.',\n",
       " 'Al Zaman: Guerrillas killed a member the Kurdistan Democratic Party after kidnapping in Mosul.',\n",
       " 'Al - Zaman: Guerrillas killed a member of the Kurdistan Democratic Party after kidnapping him in Mosul.']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise(m_sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"../dep.py\", line 106, in <module>\n",
      "    main()\n",
      "  File \"../dep.py\", line 84, in main\n",
      "    sentences = conllu.parse(f.read())\n",
      "  File \"/usr/local/anaconda3/envs/evo/lib/python3.8/site-packages/conllu/__init__.py\", line 19, in parse\n",
      "    return SentenceList(parse_incr(\n",
      "  File \"/usr/local/anaconda3/envs/evo/lib/python3.8/site-packages/conllu/models.py\", line 303, in __init__\n",
      "    sentences = list(sentences)\n",
      "  File \"/usr/local/anaconda3/envs/evo/lib/python3.8/site-packages/conllu/__init__.py\", line 39, in generator\n",
      "    yield parse_token_and_metadata(\n",
      "  File \"/usr/local/anaconda3/envs/evo/lib/python3.8/site-packages/conllu/parser.py\", line 96, in parse_token_and_metadata\n",
      "    tokens.append(parse_line(line, fields, field_parsers))\n",
      "  File \"/usr/local/anaconda3/envs/evo/lib/python3.8/site-packages/conllu/parser.py\", line 131, in parse_line\n",
      "    value = field_parsers[field](line_split, i)\n",
      "  File \"/usr/local/anaconda3/envs/evo/lib/python3.8/site-packages/conllu/parser.py\", line 15, in <lambda>\n",
      "    \"feats\": lambda line, i: parse_dict_value(line[i]),\n",
      "  File \"/usr/local/anaconda3/envs/evo/lib/python3.8/site-packages/conllu/parser.py\", line 238, in parse_dict_value\n",
      "    return {\n",
      "  File \"/usr/local/anaconda3/envs/evo/lib/python3.8/site-packages/conllu/parser.py\", line 240, in <dictcomp>\n",
      "    for part in value.split(\"|\") if parse_nullable_value(part.split(\"=\")[0]) is not None\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../dep.py ../data/ud/en_ewt-ud-train.conllu ../data/ud_train.jsonl --redundant=1 --weight=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../dep.py ../data/ud/en_ewt-ud-dev.conllu ../data/ud_dev.jsonl --redundant=3 --weight=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import TrajectoryDataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_dataset = TrajectoryDataset.from_disk(\n",
    "    path='../data/ud/ud_train.jsonl',\n",
    "    max_len=64,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "eval_dataset = TrajectoryDataset.from_disk(\n",
    "    path='../data/ud/ud_dev.jsonl',\n",
    "    max_len=64,\n",
    "    tokenizer=tokenizer,\n",
    "    limit=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data import StratifiedInfiniteSampler\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    sampler=StratifiedInfiniteSampler(train_dataset, 2),\n",
    "    collate_fn=lambda x: zip(*x)\n",
    ")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Evolver\n",
    "from torch.optim import AdamW\n",
    "\n",
    "evolver = Evolver(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    max_len=64,\n",
    "    encoder_layers=6,\n",
    "    decoder_layers=6,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "optim = AdamW(evolver.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_evolver\n",
    "\n",
    "train_evolver(\n",
    "    evolver, optim, None,\n",
    "    train_loader, eval_loader,\n",
    "    train_steps=1,\n",
    "    grad_accum_steps=1,\n",
    "    checkpoint_at=2,\n",
    "    eval_at=1,\n",
    "    num_particles=5,\n",
    "    threshold=2,\n",
    "    temperature=1.0,\n",
    "    device='cpu',\n",
    "    prefix='test-local'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import evaluate_evolver\n",
    "\n",
    "evaluate_evolver(evolver, eval_loader, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../train.py \\\n",
    "    --train ../data/ud/ud.jsonl \\\n",
    "    --eval ../data/ud/en_ewt-ud-dev.conllu \\\n",
    "    --config ../configs/ud.json \\\n",
    "    --prefix ud-1.0.0 \\\n",
    "    --device cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input/output pairs...\n",
      "done in 4.01 seconds!\n"
     ]
    }
   ],
   "source": [
    "from data import Seq2SeqDataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "dataset = Seq2SeqDataset.from_trajectories(\n",
    "    '../data/ud/ud_train.jsonl',\n",
    "    denoising=False,\n",
    "    max_len=64,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data import StratifiedInfiniteSampler\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    sampler=StratifiedInfiniteSampler(dataset, 128),\n",
    ")\n",
    "\n",
    "# just to test\n",
    "eval_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
